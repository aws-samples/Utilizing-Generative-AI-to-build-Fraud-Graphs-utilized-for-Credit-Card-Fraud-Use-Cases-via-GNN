{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c343ed8f",
   "metadata": {},
   "source": [
    "# Fraud Detection Knowledge Graph Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a comprehensive pipeline for creating a fraud detection knowledge graph using AWS services. The pipeline leverages advanced technologies to process credit card transaction data, generate a knowledge graph, and perform machine learning-based fraud detection.\n",
    "\n",
    "The pipeline consists of the following main steps:\n",
    "1. Data Preparation: Preprocessing of credit card transaction data\n",
    "2. Knowledge Graph Generation: Using Amazon Bedrock with Claude 3 Sonnet\n",
    "3. Graph Database Population: Loading data into Amazon Neptune\n",
    "4. Machine Learning: Utilizing Neptune ML with Graph Neural Networks (GNNs)\n",
    "5. Fraud Detection: Performing inferences on the graph to predict fraudulent transactions\n",
    "\n",
    "## AWS Services Used\n",
    "\n",
    "- **Amazon Bedrock**: Used for generating the knowledge graph structure from transaction data\n",
    "- **Amazon Neptune**: Graph database for storing and querying the knowledge graph\n",
    "- **Neptune ML**: Machine learning capabilities integrated with Neptune for training GNNs and performing fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb8d246",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "- An AWS account with access to:\n",
    "  - Amazon Bedrock\n",
    "  - Amazon Neptune\n",
    "  - Neptune ML\n",
    "- Python 3.x installed\n",
    "- Appropriate IAM roles and permissions for accessing AWS services:\n",
    "  - IAM role with permissions to invoke Bedrock models\n",
    "  - Neptune ML IAM role with access to S3, SageMaker, and Neptune\n",
    "  - [Neptune IAM Authentication](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth.html) configured\n",
    "- Familiarity with graph databases and machine learning concepts\n",
    "- Amazon Neptune cluster set up with ML capabilities enabled:\n",
    "  - Use the [Neptune ML Quick Start Guide](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-quick-start.html) for cluster setup\n",
    "- Model access on Amazon Bedrock for the Claude 3 Sonnet model\n",
    "\n",
    "For detailed setup instructions, refer to the [Amazon Neptune ML documentation](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning.html).\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "1. Set up AWS credentials and permissions:\n",
    "   - Ensure your AWS CLI is configured with the appropriate credentials\n",
    "   - Verify that your IAM user or role has the necessary permissions to access Bedrock, Neptune, and related services\n",
    "\n",
    "2. Install required Python libraries:\n",
    "   ```\n",
    "   pip install boto3 gremlinpython tqdm langchain langchain_experimental nest_asyncio\n",
    "   ```\n",
    "\n",
    "3. Set up Amazon Neptune cluster:\n",
    "   - Use the CloudFormation template provided in the [Neptune ML Quick Start Guide](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-quick-start.html)\n",
    "   - This will create a Neptune cluster with ML capabilities enabled\n",
    "   - Copy the `neptune_ml_utils.py` file from the path `Neptune/03-Neptune-ML/neptune_ml_utils.py` to the location of this notebook in the Neptune Notebook instance. \n",
    "\n",
    "4. Configure Bedrock access:\n",
    "   - Ensure you have model access on Bedrock for the Claude 3 Sonnet model\n",
    "   - Set up the necessary IAM permissions for Bedrock API calls\n",
    "\n",
    "Now that the setup is complete, we can proceed with the pipeline execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611500",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary dependencies. We're using a variety of libraries to interact with AWS services and process our data efficiently.\n",
    "\n",
    "- `boto3`: The AWS SDK for Python, used for interacting with various AWS services\n",
    "- `langchain` and `langchain_experimental`: For working with large language models and creating chains of operations\n",
    "- `gremlinpython`: To interact with Neptune using the Gremlin graph traversal language\n",
    "- `numpy` and `pandas`: For efficient data manipulation and analysis\n",
    "- `nest_asyncio`: To allow for asynchronous operations in Jupyter notebooks\n",
    "\n",
    "Installing these dependencies ensures we have all the tools needed for our fraud detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38138a-6bcd-49d1-ae1b-2bbb329b48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet boto3==1.34.162 langchain langchain-community langchain-experimental langchain_aws nest_asyncio json-repair awscli numpy pandas gremlinpython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e601d",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "\n",
    "Before we begin data preparation, let's understand the structure of our input data:\n",
    "\n",
    "- The input data is a CSV file containing credit card transaction information.\n",
    "- Each row represents a single transaction with fields such as transaction amount, date, merchant information, and whether the transaction was fraudulent.\n",
    "- The data may contain sensitive information, so ensure proper data handling and privacy measures are in place.\n",
    "\n",
    "### Data Structure Assumptions:\n",
    "- Transaction IDs are unique\n",
    "- Timestamps are in a consistent format\n",
    "- Monetary amounts are in a single currency\n",
    "\n",
    "### Potential Data Quality Issues:\n",
    "- Missing values in non-essential fields\n",
    "- Outliers in transaction amounts\n",
    "- Imbalanced classes (fraudulent vs. non-fraudulent transactions)\n",
    "\n",
    "We'll address these issues in our data preparation steps to ensure high-quality input for our knowledge graph generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee15333-562f-4e8a-8192-85fa258fe649",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "In this section, we'll load and preprocess the credit card transaction data. Data preparation is a crucial step in any machine learning pipeline, especially for fraud detection. We need to ensure our data is clean, properly formatted, and ready for knowledge graph generation.\n",
    "\n",
    "We'll perform the following steps:\n",
    "1. Load the raw transaction data from a CSV file\n",
    "2. Clean and preprocess the data (handling missing values, encoding categorical variables, etc.)\n",
    "3. Create features that might be relevant for fraud detection\n",
    "4. Prepare the data for input into our knowledge graph generation process\n",
    "\n",
    "This preparation will help us create a more accurate and informative knowledge graph, which is essential for effective fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a7b19-76a5-4746-b0b0-f07b715b18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"card_transaction.v1.csv\").sample(n=100000, random_state=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1900416-4b3e-44cc-84f4-d2ad7d30d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df [df['Is Fraud?'] == 'Yes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffc389-c695-4d44-8587-b5e861980c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "percent_missing=(df.isnull().sum()*100/df.shape[0]).sort_values(ascending=True)\n",
    "plt.title(\"Missing Value Analysis\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"% of missing values\")\n",
    "plt.bar(percent_missing.sort_values(ascending=False).index,percent_missing.sort_values(ascending=False),color=(0.1, 0.1, 0.1, 0.1),edgecolor='blue')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09bf75",
   "metadata": {},
   "source": [
    "### Data Cleaning and Feature Engineering\n",
    "\n",
    "Now that we've loaded our data, let's clean it up and create some useful features for our fraud detection model. This process involves:\n",
    "\n",
    "1. Handling missing values\n",
    "2. Converting data types (e.g., string to datetime)\n",
    "3. Creating new features that might be indicative of fraudulent activity\n",
    "4. Encoding categorical variables\n",
    "\n",
    "These steps will help us prepare a rich dataset for our knowledge graph and subsequent machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4440a5-6320-4219-9939-5748fc4b55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"card_id\"] = df[\"User\"].astype(str) + \"_\" + df[\"Card\"].astype(str)\n",
    "df.Amount.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96341a-0228-410e-8a0b-14a0d3a42a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Amount\"]=df[\"Amount\"].str.replace(\"$\",\"\").astype(float)\n",
    "df[\"Hour\"] = df[\"Time\"].str [0:2]\n",
    "df[\"Minute\"] = df[\"Time\"].str [3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfe21a-ae0f-4108-9c94-c132459c11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Hour.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12f5db-7323-4ddb-8a7b-9602ec3cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Minute.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfbc64d-d269-4f18-b9f5-ec86fea84485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Time\",\"User\",\"Card\"],axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21795ce-b679-45c4-993f-6f14e577ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df[\"Errors?\"].unique()\n",
    "df[\"Errors?\"]= df[\"Errors?\"].fillna(\"No error\")\n",
    "df = df.drop(columns=[\"Merchant State\",\"Zip\"],axis=1)\n",
    "df[\"Is Fraud?\"] = df[\"Is Fraud?\"].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df[\"Merchant City\"]=LabelEncoder().fit_transform(df[\"Merchant City\"])\n",
    "df[\"Use Chip\"]=LabelEncoder().fit_transform(df[\"Use Chip\"])\n",
    "df[\"Errors?\"]=LabelEncoder().fit_transform(df[\"Errors?\"])\n",
    "df[\"Errors?\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b5a65-0a81-4809-81c0-17fc49d33f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Transaction_Type': 'Transaction Type'})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49386dbd-8cb6-4255-bdb4-a00550fd8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = df.reset_index()\n",
    "df_reset.rename(columns={'index': 'Transaction_ID'}, inplace=True)\n",
    "df_reset.to_csv('purchases.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc5149-07c3-407b-b4fb-1dbcc7dc25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88216fc",
   "metadata": {},
   "source": [
    "## 2. Knowledge Graph Generation with Amazon Bedrock and Claude 3 Sonnet\n",
    "\n",
    "In this section, we use Amazon Bedrock with the Claude 3 Sonnet model to generate our knowledge graph. Here's how it works:\n",
    "\n",
    "1. Data Processing: We send batches of preprocessed transaction data to Bedrock.\n",
    "2. Natural Language Understanding: Claude 3 Sonnet analyzes the data, understanding the relationships between entities.\n",
    "3. Graph Structure Generation: The model outputs a structured representation of nodes and relationships.\n",
    "\n",
    "The resulting knowledge graph structure typically includes:\n",
    "- Nodes: Representing entities like transactions, users, and merchants\n",
    "- Relationships: Connecting nodes (e.g., \"user made transaction\", \"transaction occurred at merchant\")\n",
    "- Properties: Additional attributes on nodes and relationships\n",
    "\n",
    "Limitations and Considerations:\n",
    "- The quality of the graph depends on the input data quality and the model's understanding.\n",
    "- Large datasets may require significant processing time.\n",
    "- The model may occasionally generate inconsistent or irrelevant relationships, requiring post-processing.\n",
    "\n",
    "We'll use batch processing to handle large volumes of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efecea-c522-4a0e-9d52-2dd301f53ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import boto3\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "import nest_asyncio\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# MODEL_ID = \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "MODEL_ID = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "# MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "from botocore.config import Config\n",
    "\n",
    "# Create a configuration object with custom retry settings\n",
    "config = Config(\n",
    "    region_name = 'us-east-1',\n",
    "    retries={\n",
    "        'max_attempts': 30  # Set your desired retry count here\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize your Bedrock client with the custom config\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", config=config)\n",
    "llm = ChatBedrock(model_id=MODEL_ID, client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8f472-b80f-403b-99d4-68cbaa108a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=[\n",
    "        \"Transaction\", \n",
    "        \"Merchant\",\n",
    "        \"Card\", \n",
    "    ],\n",
    "    allowed_relationships=[\n",
    "        \"Purchased_At\", \n",
    "        \"Purchased_By\",  \n",
    "    ],\n",
    "    node_properties=[\n",
    "        \"Transaction_Type\",\n",
    "        \"Transaction_Date\", \n",
    "        \"Transaction_Time\",\n",
    "        \"Amount\",\n",
    "        \"Error_Count\",\n",
    "        \"Fraud_Status\",\n",
    "        \"Merchant_City\", \n",
    "        \"Merchant_Category\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdef27",
   "metadata": {},
   "source": [
    "### Process Data and Generate Knowledge Graph\n",
    "\n",
    "Now that we have our Bedrock client and graph transformer set up, we'll process our data to generate the knowledge graph. This involves:\n",
    "\n",
    "1. Loading our processed transaction data\n",
    "2. Splitting the data into manageable batches\n",
    "3. Using Bedrock's batch inference capabilities to process these batches in parallel\n",
    "4. Collecting and combining the results to form our complete knowledge graph\n",
    "\n",
    "This approach allows us to efficiently process large amounts of data and create a comprehensive knowledge graph representing our transaction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00f4e5-aa54-41a1-86a8-e59501beae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "file_path = (\"purchases.csv\")\n",
    "\n",
    "loader = CSVLoader(file_path=file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed19ec-f6e9-437c-a4c4-9225038c33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Any, Dict, List\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def extract_prompts(steps: List[Any]) -> List[Dict[str, Any]]:\n",
    "    def extract_prompt_template(item: Any) -> Any:\n",
    "        if hasattr(item, 'template'):\n",
    "            return item.template\n",
    "        elif hasattr(item, 'prompt'):\n",
    "            return extract_prompt_template(item.prompt)\n",
    "        elif hasattr(item, 'messages'):\n",
    "            return [extract_prompt_template(msg) for msg in item.messages]\n",
    "        elif isinstance(item, list):\n",
    "            return [extract_prompt_template(subitem) for subitem in item]\n",
    "        else:\n",
    "            return str(item)\n",
    "\n",
    "    def analyze_runnable(runnable: Any, depth: int = 0) -> Dict[str, Any]:\n",
    "        if depth > 5:  # Prevent infinite recursion\n",
    "            return {\"type\": type(runnable).__name__, \"note\": \"Max depth reached\"}\n",
    "        \n",
    "        info = {\"type\": type(runnable).__name__}\n",
    "        \n",
    "        if hasattr(runnable, 'func'):\n",
    "            info[\"func\"] = str(runnable.func)\n",
    "            if callable(runnable.func):\n",
    "                try:\n",
    "                    info[\"func_source\"] = inspect.getsource(runnable.func)\n",
    "                except Exception:\n",
    "                    info[\"func_source\"] = \"Source code not available\"\n",
    "\n",
    "        if hasattr(runnable, 'bound'):\n",
    "            info[\"bound\"] = analyze_runnable(runnable.bound, depth + 1)\n",
    "        \n",
    "        if hasattr(runnable, 'kwargs'):\n",
    "            info[\"kwargs\"] = analyze_kwargs(runnable.kwargs)\n",
    "        \n",
    "        if hasattr(runnable, 'steps'):\n",
    "            info[\"steps\"] = {k: analyze_runnable(v, depth + 1) for k, v in runnable.steps.items()}\n",
    "        \n",
    "        if hasattr(runnable, 'mapper'):\n",
    "            info[\"mapper\"] = analyze_runnable(runnable.mapper, depth + 1)\n",
    "        \n",
    "        return info\n",
    "\n",
    "    def analyze_kwargs(kwargs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        analyzed_kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, (str, int, float, bool)):\n",
    "                analyzed_kwargs[k] = v\n",
    "            elif isinstance(v, dict):\n",
    "                analyzed_kwargs[k] = analyze_kwargs(v)\n",
    "            elif isinstance(v, list):\n",
    "                analyzed_kwargs[k] = [analyze_kwargs(item) if isinstance(item, dict) else str(item) for item in v]\n",
    "            else:\n",
    "                analyzed_kwargs[k] = analyze_runnable(v)\n",
    "        return analyzed_kwargs\n",
    "\n",
    "    results = []\n",
    "    for step in steps:\n",
    "        step_info = {\"name\": step.__class__.__name__}\n",
    "        \n",
    "        if hasattr(step, 'steps__'):\n",
    "            step_info[\"type\"] = \"Complex Step\"\n",
    "            step_info[\"substeps\"] = {k: analyze_runnable(v) for k, v in step.steps__.items()}\n",
    "        elif hasattr(step, 'get_prompts'):\n",
    "            step_info[\"type\"] = \"Prompt Step\"\n",
    "            step_prompts = step.get_prompts()\n",
    "            step_info[\"prompts\"] = extract_prompt_template(step_prompts)\n",
    "        else:\n",
    "            step_info[\"type\"] = \"Unknown Step\"\n",
    "        \n",
    "        if hasattr(step, '__dict__'):\n",
    "            for key, value in step.__dict__.items():\n",
    "                if key not in ['steps__', 'prompt']:\n",
    "                    if isinstance(value, (str, int, float, bool)):\n",
    "                        step_info[key] = value\n",
    "                    elif hasattr(value, '__dict__'):\n",
    "                        step_info[key] = analyze_runnable(value)\n",
    "                    else:\n",
    "                        step_info[key] = str(value)\n",
    "        \n",
    "        results.append(step_info)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8780f-6b44-405e-bd17-d3fa59a23314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(extracted_info: List[Dict[str, Any]], input_text: str) -> str:\n",
    "    system_content = \"\"\n",
    "    human_content = \"\"\n",
    "\n",
    "    for step in extracted_info:\n",
    "        if step[\"type\"] == \"Prompt Step\" and \"prompts\" in step:\n",
    "            for prompt in step[\"prompts\"]:\n",
    "                if isinstance(prompt, list):\n",
    "                    system_content += prompt[0] + \"\\n\\n\"\n",
    "                    human_content += prompt[1] + \"\\n\\n\"\n",
    "                elif isinstance(prompt, str):\n",
    "                    system_content += prompt + \"\\n\\n\"\n",
    "        elif step[\"type\"] == \"Complex Step\" and \"substeps\" in step:\n",
    "            for substep in step[\"substeps\"].values():\n",
    "                if substep[\"type\"] == \"RunnableBinding\" and \"kwargs\" in substep:\n",
    "                    tools = substep[\"kwargs\"].get(\"tools\", [])\n",
    "                    for tool in tools:\n",
    "                        system_content += f\"Use the following tool: {json.dumps(tool)}\\n\\n\"\n",
    "\n",
    "    # Prepare messages as a list of dictionaries (JSON array)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": human_content.strip().format(input=input_text)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Certainly, I'll extract the information from the provided input and create a knowledge graph according to the instructions. I'll use the specified tools to structure the output. Here's the extracted information:\"}\n",
    "    ]\n",
    "\n",
    "    return system_content.strip(), messages\n",
    "\n",
    "def process_langchain_to_prompt(llm_transformer_filtered, input_text):\n",
    "    extracted_info = extract_prompts(llm_transformer_filtered.chain.steps)\n",
    "    prompt_messages = prepare_prompt(extracted_info, input_text)\n",
    "    return prompt_messages\n",
    "\n",
    "def call_bedrock_claude(system_message, prompt_messages):\n",
    "    # Initialize the Bedrock client\n",
    "    bedrock = boto3.client('bedrock-runtime')\n",
    "\n",
    "    # Prepare the request body\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"system\":system_message,\n",
    "        \"messages\": prompt_messages,  # Use the list of message dictionaries directly\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "\n",
    "    # Make the API call\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId='anthropic.claude-3-sonnet-20240229-v1:0',  # Use the appropriate model ID\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps(body)\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['content'][0]['text']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example of how to use the functions together (commented out)\n",
    "input_text = \"transaction_id: 16966740\\nYear: 2006\\nMonth: 5\\nDay: 13\\nAmount: 96.0\\nTransaction Type: 2\\nMerchant Name: 1799189980464955940\\nMerchant City: 4954\\nMCC: 5499\\nErrors?: 10\\nIs Fraud?: 0\\ncard_id: 1378_2\\ntime: 09:08:00\"\n",
    "\n",
    "system_message, prompt_messages = process_langchain_to_prompt(llm_transformer, input_text)\n",
    "print(f\"Generated JSON:\\n{system_message, prompt_messages}\")\n",
    "result = call_bedrock_claude(system_message, prompt_messages)\n",
    "\n",
    "if result:\n",
    "    print(\"Generated Knowledge Graph:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Failed to generate knowledge graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0edf2-0990-492b-bdf6-6983f5d103c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_jsonl(extracted_info: List[Dict[str, Any]], input_text: str) -> Dict[str, Any]:\n",
    "    messages = []\n",
    "    tools = []\n",
    "    system_content = \"\"\n",
    "\n",
    "    for step in extracted_info:\n",
    "        if step[\"type\"] == \"Prompt Step\" and \"prompts\" in step:\n",
    "            for prompt in step[\"prompts\"]:\n",
    "                if isinstance(prompt, list):\n",
    "                    system_content += prompt[0] + \"\\n\\n\"\n",
    "                    messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": prompt[1].format(input=input_text)}]\n",
    "                    })\n",
    "                elif isinstance(prompt, str):\n",
    "                    system_content += prompt + \"\\n\\n\"\n",
    "        elif step[\"type\"] == \"Complex Step\" and \"substeps\" in step:\n",
    "            for substep in step[\"substeps\"].values():\n",
    "                if substep[\"type\"] == \"RunnableBinding\" and \"kwargs\" in substep:\n",
    "                    tools.extend(substep[\"kwargs\"].get(\"tools\", []))\n",
    "\n",
    "    # Create the output in the specified format\n",
    "    output = {\n",
    "        \"recordId\": str(uuid.uuid4()).replace('-','')[:11],  # Generate a unique 11-character ID\n",
    "        \"modelInput\": {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 4000,\n",
    "            \"system\": system_content.strip(),\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Only add tools if they exist\n",
    "    if tools:\n",
    "        output['modelInput'][\"tools\"] = [\n",
    "            {\n",
    "                \"name\": tool.get(\"name\", \"\"),\n",
    "                \"description\": tool.get(\"description\", \"\"),\n",
    "                \"input_schema\": tool.get(\"input_schema\", {})  \n",
    "            }\n",
    "            for tool in tools\n",
    "        ]\n",
    "        # Adding a default tool_choice\n",
    "        output['modelInput'][\"tool_choice\"] = {\"type\": \"auto\"}\n",
    "\n",
    "    return output\n",
    "\n",
    "def process_langchain_documents_to_prompt_jsonl(llm_transformer_filtered, documents, output_file):\n",
    "    extracted_info = extract_prompts(llm_transformer_filtered.chain.steps)\n",
    "    with open(output_file, 'w') as f:\n",
    "        for doc in documents:\n",
    "            prompt_message = prepare_prompt_jsonl(extracted_info, doc.page_content)\n",
    "            \n",
    "            # Write the JSON to the file, followed by a newline\n",
    "            json.dump(prompt_message, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    print(f\"Prompts have been saved to {output_file}\")\n",
    "\n",
    "# process_langchain_documents_to_prompt_jsonl(llm_transformer, documents[:1000], 'batch-1k.jsonl')\n",
    "# process_langchain_documents_to_prompt_jsonl(llm_transformer, documents[:30000], 'batch-30k.jsonl')\n",
    "# process_langchain_documents_to_prompt_jsonl(llm_transformer, documents[:45000], 'batch-45k.jsonl')\n",
    "process_langchain_documents_to_prompt_jsonl(llm_transformer, documents[:50000], 'batch-50k.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd7c43-1cf7-4d2c-b998-b942b6635945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aacd71-27d5-4ff0-95e7-5722f4f6ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c0552",
   "metadata": {},
   "source": [
    "## 3. Graph Database Population\n",
    "\n",
    "After generating our knowledge graph, the next step is to load this data into Amazon Neptune, our graph database. This process involves:\n",
    "\n",
    "1. Preparing the generated nodes and relationships for Neptune ingestion\n",
    "2. Configuring the Neptune connection\n",
    "3. Using the Neptune bulk loader to efficiently insert large amounts of data\n",
    "4. Verifying the data load and performing basic queries\n",
    "\n",
    "Loading our knowledge graph into Neptune allows us to leverage its powerful query capabilities and sets the stage for our machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdf25d-efb7-40fd-91cf-5840559cf4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "from collections import deque\n",
    "\n",
    "# Initialize clients\n",
    "s3 = boto3.client('s3')\n",
    "bedrock = boto3.client('bedrock')\n",
    "\n",
    "# S3 bucket names\n",
    "input_bucket = 'bedrock-batch-inference-fraud-detection'\n",
    "output_bucket = 'bedrock-batch-inference-fraud-detection'\n",
    "\n",
    "# IAM role ARN\n",
    "role_arn = 'arn:aws:iam::590183881541:role/Bedrock-S3-FullAccess'\n",
    "\n",
    "def read_jsonl_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def create_jsonl_shard(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "def upload_to_s3(filename, bucket, s3_key):\n",
    "    s3.upload_file(filename, bucket, s3_key)\n",
    "\n",
    "def download_from_s3(bucket, s3_key, filename):\n",
    "    s3.download_file(bucket, s3_key, filename)\n",
    "\n",
    "def check_job_status(job_identifier):\n",
    "    response = bedrock.get_model_invocation_job(jobIdentifier=job_identifier)\n",
    "    return response['status'] # 'Submitted'|'InProgress'|'Completed'|'Failed'|'Stopping'|'Stopped'|'PartiallyCompleted'|'Expired'|'Validating'|'Scheduled'\n",
    "\n",
    "def submit_job(shard_data, shard_index):\n",
    "    unique_job_name = f\"batch-job-{uuid.uuid4()}-shard-{shard_index}\"\n",
    "    shard_filename = f'input_shard_{shard_index}.jsonl'\n",
    "    \n",
    "    create_jsonl_shard(shard_data, shard_filename)\n",
    "    s3_input_key = f'input/{shard_filename}'\n",
    "    upload_to_s3(shard_filename, input_bucket, s3_input_key)\n",
    "    \n",
    "    input_data_config = {\n",
    "        \"s3InputDataConfig\": {\n",
    "            \"s3Uri\": f\"s3://{input_bucket}/{s3_input_key}\"\n",
    "        }\n",
    "    }\n",
    "    output_data_config = {\n",
    "        \"s3OutputDataConfig\": {\n",
    "            \"s3Uri\": f\"s3://{output_bucket}/\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock.create_model_invocation_job(\n",
    "        roleArn=role_arn,\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        jobName=unique_job_name,\n",
    "        inputDataConfig=input_data_config,\n",
    "        outputDataConfig=output_data_config\n",
    "    )\n",
    "    job_identifier = response['jobArn']\n",
    "    \n",
    "    print(f\"Job submitted for shard {shard_index}. Name: {unique_job_name}, Identifier: {job_identifier}\")\n",
    "    \n",
    "    os.remove(shard_filename)\n",
    "    \n",
    "    return job_identifier, unique_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddd4e3-940b-45ea-bbc6-13531d99c7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shard configuration\n",
    "shard_size = 10000  # Number of items per shard\n",
    "\n",
    "# Input JSONL file\n",
    "input_jsonl_file = 'batch-50k.jsonl'  # Replace with your input file name\n",
    "\n",
    "# Read the input JSONL file\n",
    "data = read_jsonl_file(input_jsonl_file)\n",
    "print(f\"Read {len(data)} items from {input_jsonl_file}\")\n",
    "\n",
    "# Shard the data\n",
    "shards = [data[i:i + shard_size] for i in range(0, len(data), shard_size)]\n",
    "print(f\"Created {len(shards)} shards\")\n",
    "\n",
    "# Queue to hold all shards\n",
    "shard_queue = deque(enumerate(shards))\n",
    "\n",
    "# List to hold active jobs\n",
    "active_jobs = []\n",
    "\n",
    "# List to hold completed jobs\n",
    "completed_jobs = []\n",
    "\n",
    "# Main loop\n",
    "while shard_queue or active_jobs:\n",
    "    # Submit new jobs if there are fewer than 3 active jobs and shards are available\n",
    "    while len(active_jobs) < 3 and shard_queue:\n",
    "        shard_index, shard = shard_queue.popleft()\n",
    "        job_identifier, job_name = submit_job(shard, shard_index)\n",
    "        active_jobs.append((shard_index, job_identifier, job_name))\n",
    "\n",
    "    # Check status of active jobs\n",
    "    for job in active_jobs[:]:  # Iterate over a copy of the list\n",
    "        shard_index, job_identifier, job_name = job\n",
    "        status = check_job_status(job_identifier)\n",
    "        if status in ['Completed', 'Failed', 'Stopped']:\n",
    "            print(f\"Job for shard {shard_index} (Name: {job_name}) finished with status: {status}\")\n",
    "            completed_jobs.append((shard_index, status, job_identifier, job_name))\n",
    "            active_jobs.remove(job)\n",
    "        else:\n",
    "            print(f\"Job for shard {shard_index} (Name: {job_name}) with status: {status}\")\n",
    "\n",
    "    # Wait before checking again\n",
    "    if active_jobs:\n",
    "        print(f\"Waiting for {len(active_jobs)} jobs to complete...\")\n",
    "        time.sleep(60)  # Wait for 60 seconds before checking again\n",
    "\n",
    "# Save completed jobs to a JSON file\n",
    "with open('completed_jobs.json', 'w+') as f:\n",
    "    json.dump(completed_jobs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d6b8f-5a41-4a29-b2f3-a4e253271fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "from collections import deque\n",
    "\n",
    "# Initialize clients\n",
    "s3 = boto3.client('s3')\n",
    "bedrock = boto3.client('bedrock')\n",
    "completed_jobs = []\n",
    "\n",
    "\n",
    "output_bucket = 'bedrock-batch-inference-fraud-detection'\n",
    "\n",
    "def download_from_s3(bucket, s3_key, filename):\n",
    "    s3.download_file(bucket, s3_key, filename)\n",
    "\n",
    "# Load completed jobs from a JSON file\n",
    "try:\n",
    "    with open('completed_jobs.json', 'r') as f:\n",
    "        completed_jobs = json.load(f)\n",
    "    print(f\"Loaded {len(completed_jobs)} completed jobs from 'completed_jobs.json'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No 'completed_jobs.json' file found, starting with an empty list of completed jobs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f56614-0197-4750-be61-cd0f3971d451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "nodes = []\n",
    "relationships = []\n",
    "for shard_index, status, job_identifier, job_name in completed_jobs:\n",
    "    if status == 'Completed':\n",
    "        output_file = f'input_shard_{shard_index}.jsonl.out'\n",
    "        s3_output_key = f'{job_identifier.split(\"/\")[-1]}/{output_file}'\n",
    "        download_from_s3(output_bucket, s3_output_key, output_file)\n",
    "        print(f\"Results for shard {shard_index} (Job: {job_name}) downloaded to {output_file}\")\n",
    "        with open(output_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    jsonl = json.loads(line)\n",
    "                    modelOutput = jsonl['modelOutput']['content']\n",
    "                    for e in modelOutput:\n",
    "                        tool_input = e['input']\n",
    "                        if isinstance(tool_input, dict) and 'nodes' in tool_input and 'relationships' in tool_input:\n",
    "                            nodes += tool_input['nodes']\n",
    "                            relationships += tool_input['relationships']\n",
    "                    count+=1\n",
    "                except (ValueError, KeyError):\n",
    "                    # Ignore lines not in the expected format\n",
    "                    pass\n",
    "        os.remove(output_file)\n",
    "    else:\n",
    "        print(f\"Job for shard {shard_index} (Name: {job_name}, Identifier: {job_identifier}) did not complete successfully. Check the AWS console for more details.\")\n",
    "\n",
    "print(f\"{count} records transformed\")\n",
    "print(f\"Total Nodes: {len(nodes)}\", f\"Node Example: {nodes[0]}\")\n",
    "print(f\"Total Relationships: {len(relationships)}\", f\"Relationship Example: {relationships[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45792b1e-6ef1-4a1d-963c-715c45ea63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41dc1a-37a2-4d55-9058-72807908038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet gremlinpython tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317827c-d284-4b8b-8d78-3ee10dda25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from gremlin_python.driver import client, serializer\n",
    "# Your Neptune endpoint and port\n",
    "neptune_endpoint = 'neptunedbcluster-dhcc2z0nwwe2.cluster-cl6ea8ky45tr.us-east-1.neptune.amazonaws.com'\n",
    "neptune_port = '8182'\n",
    "\n",
    "# Create a Neptune client\n",
    "neptune_client = client.Client(\n",
    "    f'wss://{neptune_endpoint}:{neptune_port}/gremlin',\n",
    "    'g',\n",
    "    message_serializer=serializer.GraphSONSerializersV2d0()\n",
    ")\n",
    "\n",
    "print(neptune_client.submit(\"g.E().drop()\").all().result())\n",
    "print(neptune_client.submit(\"g.V().drop()\").all().result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4cef5-f973-4145-af5f-ecea84385532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from gremlin_python.driver import client, serializer\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create a Neptune client\n",
    "neptune_client = client.Client(\n",
    "    f'wss://{neptune_endpoint}:{neptune_port}/gremlin',\n",
    "    'g',\n",
    "    message_serializer=serializer.GraphSONSerializersV2d0()\n",
    ")\n",
    "\n",
    "def add_to_neptune(nodes, relationships):\n",
    "    try:\n",
    "        # Upsert nodes\n",
    "        for node in tqdm(nodes):\n",
    "            query = (\n",
    "                f\"g.V('{node['id']}').fold().coalesce(\"\n",
    "                f\"unfold(), \"\n",
    "                f\"addV('{node['type']}').property(id, '{node['id']}')\"\n",
    "                f\")\"\n",
    "            )\n",
    "            \n",
    "            for property in node.get('properties',[]):\n",
    "                query += f\".property(single, '{property['key']}', {repr(property['value'])})\"\n",
    "            \n",
    "            # logger.info(f\"Upserting node: {node['id']}\")\n",
    "            result = neptune_client.submit(query).all().result()\n",
    "            logger.debug(f\"Node upsert result: {result}\")\n",
    "\n",
    "        # Upsert relationships\n",
    "        for rel in tqdm(relationships):\n",
    "            query = (\n",
    "                f\"g.V('{rel['source_node_id']}').as('source')\"\n",
    "                f\".V('{rel['target_node_id']}').as('target')\"\n",
    "                f\".coalesce(\"\n",
    "                f\"__.inE('{rel['type']}').where(outV().as('source')),\"\n",
    "                f\"addE('{rel['type']}').from('source').to('target')\"\n",
    "                f\")\"\n",
    "            )\n",
    "            for property in rel.get('properties',[]):\n",
    "                query += f\".property('{property['key']}', {repr(property['value'])})\"\n",
    "\n",
    "            # logger.info(f\"Upserting relationship: {rel['type']} from {rel['source_node_id']} to {rel['target_node_id']}\")\n",
    "            result = neptune_client.submit(query).all().result()\n",
    "            logger.debug(f\"Relationship upsert result: {result}\")\n",
    "\n",
    "        logger.info(f\"Graph data upserted successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while upserting graph data: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130ceaf-33a5-4a49-945b-5de76650730b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_to_neptune(nodes, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbb5f6-62ed-403f-b05f-e47b02dbd921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.V().groupCount().by(label).unfold().order().by(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e20c98-2afb-4041-8536-9ea05ec7f9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.E().groupCount().by(label).unfold().order().by(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316f60e",
   "metadata": {},
   "source": [
    "## Graph Neural Networks (GNNs) and Fraud Detection\n",
    "\n",
    "Before we set up Neptune ML, let's understand why Graph Neural Networks are particularly suitable for fraud detection:\n",
    "\n",
    "### What are GNNs?\n",
    "Graph Neural Networks are a class of deep learning models designed to work directly on graph-structured data. They can capture complex patterns and relationships within interconnected data.\n",
    "\n",
    "### Why GNNs for Fraud Detection?\n",
    "1. Relational Information: GNNs can leverage the relationships between entities (e.g., users, transactions, merchants) to identify suspicious patterns.\n",
    "2. Feature Propagation: They can aggregate information from neighboring nodes, capturing broader context beyond individual transactions.\n",
    "3. Structural Patterns: GNNs can learn and identify subgraph patterns that may indicate fraudulent activity.\n",
    "\n",
    "### Neptune ML and SageMaker Integration\n",
    "Neptune ML integrates with Amazon SageMaker to train and deploy GNN models:\n",
    "1. Neptune exports graph data to a format suitable for ML training.\n",
    "2. SageMaker runs the GNN training process using the exported data.\n",
    "3. Trained models are deployed as SageMaker endpoints for inference.\n",
    "\n",
    "### Our Specific ML Problem\n",
    "We're tackling a node classification problem:\n",
    "- Goal: Predict whether a transaction node is fraudulent or not.\n",
    "- Input: Transaction node features and its local graph structure.\n",
    "- Output: Probability of the transaction being fraudulent.\n",
    "\n",
    "This approach allows us to leverage both the transaction details and the broader context of the user's and merchant's transaction history for more accurate fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89150a",
   "metadata": {},
   "source": [
    "## 4. Neptune ML Setup\n",
    "\n",
    "With our data loaded into Neptune, we can now leverage Neptune ML to perform machine learning tasks on our graph. Neptune ML integrates with Amazon SageMaker to train and deploy Graph Neural Network (GNN) models. In this section, we'll:\n",
    "\n",
    "1. Configure Neptune ML settings\n",
    "2. Prepare our data for machine learning tasks\n",
    "3. Define our ML problem (node classification for fraud detection)\n",
    "4. Set up the necessary IAM roles and permissions\n",
    "\n",
    "This setup lays the groundwork for training our fraud detection model using the structure and features of our knowledge graph.\n",
    "\n",
    "NOTE: Copy the `neptune_ml_utils.py` file from the path `Neptune/03-Neptune-ML/neptune_ml_utils.py` to the location of this notebook in the Neptune Notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a664e-97ce-4bdd-a61d-17ff4edef01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.V().hasLabel('Transaction').has('fraud_status', \"0\").limit(100).property('actual_fraud_status', \"0\").iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2c960-d62f-418e-a05d-9050014f3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.V().hasLabel('Transaction').has('fraud_status', \"1\").limit(100).property('actual_fraud_status', \"1\").iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b24ee8-24bf-43cc-93ed-01cb75cb9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.V().hasLabel('Transaction').has('actual_fraud_status').properties('fraud_status').drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c5992-661a-41d9-ba40-49a8d3cc264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "g.V().hasLabel('Transaction').hasNot('fraud_status').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e35714-967f-4b6d-a485-edcf5338f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_uri=\"s3://neptune-data-fraud-detection\"\n",
    "# remove trailing slashes\n",
    "s3_bucket_uri = s3_bucket_uri[:-1] if s3_bucket_uri.endswith('/') else s3_bucket_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b900ece-1278-4e6b-86ad-1b580d21b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune_ml_utils as neptune_ml\n",
    "neptune_ml.check_ml_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50348086-fc0a-4ddc-9e4d-83176a0c4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_params={ \n",
    "\"command\": \"export-pg\", \n",
    "\"params\": { \"endpoint\": neptune_ml.get_host(),\n",
    "            \"profile\": \"neptune_ml\",\n",
    "            \"useIamAuth\": neptune_ml.get_iam(),\n",
    "            \"cloneCluster\": True,\n",
    "            \"cloneClusterInstanceType\": \"r5.12xlarge\",\n",
    "            \"nodeLabels\": [\"Card\",\"Transaction\", \"Merchant\"],\n",
    "            \"edgeLabels\": [\"PURCHASED_BY\", \"PURCHASED_AT\"] \n",
    "            }, \n",
    "\"outputS3Path\": f'{s3_bucket_uri}/neptune-export',\n",
    "\"additionalParams\": {\n",
    "        \"neptune_ml\": {\n",
    "          \"version\": \"v2.0\",\n",
    "          \"targets\": [\n",
    "            {\n",
    "              \"node\": \"Transaction\",\n",
    "              \"property\": \"fraud_status\",\n",
    "              \"type\": \"classification\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "\"jobSize\": \"medium\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562c082-4e9a-4e9d-8590-2011bb1600b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\n",
    "${export_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d8735-a74b-4940-a725-a3d9b6f0ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_job_name = neptune_ml.get_training_job_name('node-classification')+'-3'\n",
    "outputS3Uri = export_results['outputS3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125ab03-be07-4a47-b786-2b4e687755a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_params = f\"\"\"\n",
    "--config-file-name training-data-configuration.json\n",
    "--job-id {training_job_name} \n",
    "--s3-input-uri {outputS3Uri} \n",
    "--s3-processed-uri {str(s3_bucket_uri)}/preloading \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd20a2-9bd8-4d43-82f2-7221c190b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%neptune_ml dataprocessing start --wait --store-to processing_results {processing_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134ed7",
   "metadata": {},
   "source": [
    "## 5. Model Training and Deployment\n",
    "\n",
    "In this section, we'll train our Graph Neural Network model for fraud detection and deploy it for inference. The process includes:\n",
    "\n",
    "1. Initiating the Neptune ML training job\n",
    "2. Monitoring the training process and evaluating results\n",
    "3. Deploying the trained model as an endpoint\n",
    "4. Testing the deployed model with sample queries\n",
    "\n",
    "This step transforms our knowledge graph into an actionable fraud detection system, capable of identifying potentially fraudulent transactions based on complex patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9ba95-d0b9-4249-9423-e67be5a3972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params=f\"\"\"\n",
    "--job-id {training_job_name} \n",
    "--data-processing-id {training_job_name}\n",
    "--instance-type ml.m5.large\n",
    "--s3-output-uri {str(s3_bucket_uri)}/training \n",
    "--max-hpo-number 2\n",
    "--max-hpo-parallel 2 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122b6e6-c194-4e31-aa03-f3e2169ae8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%neptune_ml training start --wait --store-to training_results {training_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77337f0-d07f-426a-bdc5-d1fe22872ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_params=f\"\"\"\n",
    "--id {training_job_name}\n",
    "--model-training-job-id {training_job_name}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4fe74-a8cf-40f0-a493-db406a01a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "%neptune_ml endpoint create --wait --store-to endpoint_results {endpoint_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c229093-5f2d-48eb-850d-ff645ad35e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint=endpoint_results['endpoint']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5503c27",
   "metadata": {},
   "source": [
    "## 6. Fraud Detection Inference\n",
    "\n",
    "Now that our model is trained and deployed, we can use it to detect potentially fraudulent transactions. In this section, we'll:\n",
    "\n",
    "1. Prepare sample transaction data for inference\n",
    "2. Send queries to our deployed model endpoint\n",
    "3. Interpret the model's predictions\n",
    "4. Discuss how to integrate this system into a real-time fraud detection pipeline\n",
    "\n",
    "We'll also explore how to analyze the model's performance and iterate on our approach to improve fraud detection accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c78390-747b-47ef-9a46-b0278c5dc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin --store-to transductive_id\n",
    "\n",
    "g.V().hasLabel('Transaction').hasNot('fraud_status').limit(1).id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6b3e2-6140-4f77-8d4a-ec609d8de9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin \n",
    "\n",
    "g.V(${transductive_id}).properties(\"fraud_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90357ce-25a3-4814-83ae-c6ee4007329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.with(\"Neptune#ml.endpoint\", \"${endpoint}\")\n",
    "    .V(${transductive_id}).properties('fraud_status', 'Neptune#ml.score')\n",
    "    .with(\"Neptune#ml.classification\")\n",
    "    .value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08a24b-b68f-4033-9a23-4c9807b658b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "g.with(\"Neptune#ml.endpoint\", \"${endpoint}\")\n",
    "    .V(${transductive_id}).project('actual_fraud', 'predictedFraud')\n",
    "    .by(values('actual_fraud_status').fold())\n",
    "    .by(properties('fraud_status')\n",
    "    .with(\"Neptune#ml.classification\").value().fold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca49ca-9405-489c-a822-76b0dda65b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin --store-to predictions\n",
    "\n",
    "g.with(\"Neptune#ml.endpoint\", \"${endpoint}\")\n",
    "    .V().hasLabel('Transaction').hasNot('fraud_status').project('actual_fraud', 'predictedFraud')\n",
    "    .by(values('actual_fraud_status').fold())\n",
    "    .by(properties('fraud_status').with(\"Neptune#ml.classification\").value().fold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227b9ec-b8e4-4bd4-b1ff-ed1d9f03dc49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# neptune_ml.delete_endpoint(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89d52a",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've walked through the entire process of building a fraud detection system using knowledge graphs and graph neural networks. We've covered:\n",
    "\n",
    "1. Data preparation and feature engineering\n",
    "2. Knowledge graph generation using Amazon Bedrock\n",
    "3. Loading data into Amazon Neptune\n",
    "4. Setting up and using Neptune ML for fraud detection\n",
    "5. Model training, deployment, and inference\n",
    "\n",
    "To further improve this system, consider:\n",
    "\n",
    "- Incorporating additional data sources to enrich the knowledge graph\n",
    "- Experimenting with different GNN architectures and hyperparameters\n",
    "- Implementing a real-time data ingestion and fraud detection pipeline\n",
    "- Developing a user interface for fraud analysts to interact with the system\n",
    "\n",
    "Remember to monitor your AWS resource usage and clean up any unnecessary resources to avoid unexpected costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
